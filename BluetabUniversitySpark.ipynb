{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-837VJMV:4041\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1629995619840)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfCSV: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfCSV = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"C:/Users/BLUETAB/Documents/Bluetab/BluetabUniversity/SesionSpark/bluetabUniversitySpark/clientes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfReal: org.apache.spark.sql.DataFrame = [id: string, first_name: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfReal = dfCSV.withColumn(\"profession\", regexp_replace(col(\"profession\"),\"[1]\",\"l\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfCast: org.apache.spark.sql.DataFrame = [id: string, email: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfCast = dfCSV.select(col(\"id\"), col(\"email\"), col(\"debt\").cast(\"decimal(15,2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- profession: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- register_date: string (nullable = true)\n",
      " |-- partition_field: string (nullable = true)\n",
      " |-- debt: decimal(15,2) (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCSV.withColumn(\"debt\", col(\"debt\").cast(\"decimal(15,2)\")).printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">1. Lectura</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">1.1 Leer CSV</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">1.2 Leer AVRO</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [profession: string, Category: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.format(\"avro\").\n",
    "load(\"C:/Users/BLUETAB/Documents/Bluetab/BluetabUniversity/SesionSpark/bluetabUniversitySpark/profession/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---+----------+\n",
      "|    profession|Category| Sg|     fecha|\n",
      "+--------------+--------+---+----------+\n",
      "|        doctor|      AA| Dr|26/08/2021|\n",
      "|police officer|       B| PO|26/08/2021|\n",
      "|   firefighter|       C| FF|26/08/2021|\n",
      "|        worker|       D| Wr|26/08/2021|\n",
      "|     developer|       B| Dv|26/08/2021|\n",
      "|         other|       F| Ot|26/08/2021|\n",
      "+--------------+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"fecha\", fil6/08/2021\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+----------+---+------------+--------+------------+-------------+---------------+-----------+------+----------+--------+---+\n",
      "| id|first_name|last_name|               email|profession|age|        city| country|country_code|register_date|partition_field|       debt|salary|profession|Category| Sg|\n",
      "+---+----------+---------+--------------------+----------+---+------------+--------+------------+-------------+---------------+-----------+------+----------+--------+---+\n",
      "|100|   Aigneis| Sinegold|Aigneis.Sinegold@...| developer| 65|     Hanover|   Gabon|          AI|   2014-08-22|            003| 9855840.38|     E| developer|       B| Dv|\n",
      "|102|Marguerite|     Zola|Marguerite.Zola@m...| developer| 21|     Tripoli| Burundi|          JM|   2013-07-02|            001| 9172087.55|     F| developer|       B| Dv|\n",
      "|104|     Talya| Shirberg|Talya.Shirberg@me...|     other| 38|Philadelphia|Suriname|          SG|   2014-04-19|            001|83293969.58|     H|     other|       F| Ot|\n",
      "+---+----------+---------+--------------------+----------+---+------------+--------+------------+-------------+---------------+-----------+------+----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfReal.filter(col(\"age\")<70).join(df2, dfReal(\"profession\")===df2(\"profession\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">1.3 Leer PARQUET</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfParquet: org.apache.spark.sql.DataFrame = [cutoff_date: date, entity_id: string ... 91 more fields]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfParquet = spark.read.parquet(\"C:/Users/BLUETAB/Documents/Bluetab/BluetabUniversity/SesionSpark/bluetabUniversitySpark/contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfParquet2: org.apache.spark.sql.DataFrame = [profession: string, Sg: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfParquet2 = spark.read.parquet(\"C:/Users/BLUETAB/Documents/Bluetab/BluetabUniversity/SesionSpark/bluetabUniversitySpark/contractsP/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profession: string (nullable = true)\n",
      " |-- Sg: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfParquet2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"overwrite\").partitionBy(\"Category\").parquet(\"C:/Users/BLUETAB/Documents/Bluetab/BluetabUniversity/SesionSpark/bluetabUniversitySpark/contractsP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [contract_id: string, fecha: date ... 1 more field]\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = dfParquet.select(col(\"contract_id\"), when(col(\"next_review_date\").equalTo(\"0001-01-01\"), null)\n",
    "        .otherwise(col(\"next_review_date\")).as(\"fecha\"), date_format(col(\"next_review_date\"),\"dd-MM-yyyy\").alias(\"fecha_casteada\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|fecha_casteada|\n",
      "+--------------+\n",
      "|    22-07-2019|\n",
      "|    13-12-2019|\n",
      "|    14-09-2019|\n",
      "|    17-07-2019|\n",
      "|    25-09-2019|\n",
      "|    28-07-2019|\n",
      "|    01-02-2021|\n",
      "|    12-10-2017|\n",
      "|    08-08-2019|\n",
      "|    13-07-2019|\n",
      "|    20-02-2020|\n",
      "|    20-08-2019|\n",
      "|    15-07-2019|\n",
      "|    16-07-2019|\n",
      "|    10-09-2019|\n",
      "|    30-07-2019|\n",
      "|    26-12-2019|\n",
      "|    29-08-2019|\n",
      "|    23-07-2019|\n",
      "|    19-08-2019|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(\"fecha_casteada\").distinct.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">2. Conocer los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">2.1 Conocer el schema</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">2.2 Consultar los datos</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">3. Limpieza de datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">3.1 Hay valores nulos?</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">3.2 Hay valores duplicados (distinct, group By, dropDuplicates)?</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|       contract_id|cantidad|\n",
      "+------------------+--------+\n",
      "|001304014900194188|       2|\n",
      "|001305894900004886|       2|\n",
      "|001306204800069286|       2|\n",
      "|001303034900044184|       2|\n",
      "|001304014900187687|       2|\n",
      "|001305004601152086|       2|\n",
      "|001304014900177282|       2|\n",
      "|001303054701336082|       4|\n",
      "|001306204800068981|       2|\n",
      "|001305005400501382|       2|\n",
      "|001305004601154678|       2|\n",
      "|001304015000026778|       2|\n",
      "|001306914800001481|       2|\n",
      "|001308335400107881|       2|\n",
      "|001304014900190681|       2|\n",
      "|001306204800071076|       2|\n",
      "|001305005400501572|       2|\n",
      "|001303054900052177|       2|\n",
      "|001304010200002274|       1|\n",
      "|001304015000037072|       1|\n",
      "+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfParquet.select(\"contract_id\").groupBy(\"contract_id\").agg(count(\"contract_id\").as(\"cantidad\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Long = 23231\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfParquet.select(\"contract_id\").dropDuplicates.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">3.3 Formatear la fecha (dd/mm/yyyy)</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">3.4 Castear datos</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">4. Cruzar dataframes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-family: 'Bebas Neue'; font-size: 2em;\">4.1 Cambiar nombre de columnas (as and column rename)</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">5. Filtrar Datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">6. Crear nuevos dataFrames</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">7. Particionar</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">8. Agrupar</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">9. Crear nuevas columas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">10. Borrar columas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">11. Concatenar columas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">12. Ordenmiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">13. Limitar</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-family: 'Bebas Neue'; font-size: 3em;\">14. Expresiones regulares</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
